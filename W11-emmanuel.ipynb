{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS 421 PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you will be working with data extracted from famous recommender systems type datasets: you are provided with a large set of interactions between users (persons)  and items (movies). Whenever a user \"interacts\" with an item, it watches the movie and gives a mark or \"rating\" between 1 and 5 stars (5 stars indicating that the user liked that movie very much, and 1 star indicating that the user didn't like the movie at all. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will **not** be performing the recommendation task per se. Instead, we will identify *anomalous users*. In the dataset that you are provided with, some of the data was corrupted. Whilst most of the data comes from real life user-item interactions from a famous movie rating website, some \"users\" are anomalous: they were generated by me according to some undisclosed procedure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are provided with two data frames: the first one (\"ratings\") contains the interactions provided to you, and the second one (\"labels\") contains the labels for the users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the three columns in \"ratings\" correspond to the user ID, the item ID and the rating. Thus, each row of \"ratings\" contains a single interaction. For instance, if the row \"142, 152, 5\" is present, this means that the user with ID 142 has given the movie 152 the rating 5 stars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe \"labels\" has two columns. In the first column we have the user ids, whilst the second column contains the labels. A label of 1 indicates that the user is fake (generated by me), whilst a label of 0 denotes a natural user (coming from real life interactions). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, if the labels matrix contains the line \"142, 1\", it means that all of the ratings given by the user with id 142 are fake. This means all lines in the dataframe \"ratings\" which start with the userID 142 correspond to fake interactions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to be able to classify unseen instances as either anomalies or non anomalies (guess whether they are real users or if they were generated by me). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **far more** normal users than anomalies in the dataset, which makes this a very heavily **unbalanced dataset**. Thus, accuracy will not be a good measure of performance, since simply predicting that every user is normal will give good accuracy. Thus, we need to use some other evaluation metrics (see lecture notes from week 3). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE **EVALUATION METRICS** are:  THE **AUC** (AREA UNDER CURVE), the **PRECISION**, THE **RECALL**, and the **F1 score**. The **main metric** will be the **AREA UNDER CURVE**, and it will by default be used to rank teams. This means your programs should return an **anomaly score** for each user (the higher the score, the more likely the model think the sample is anomalous).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every few weeks, we will evaluate the performance of each team (on an *unseen test set* I will provide) in terms of AUC, PRECISION, RECALL and F1 score, and rank the teams by **AUC** and by F1 score to distinguish between ties, where a tie is defined by a difference of less than 0.005 in AUC.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difficulty implied by **the generation procedure of the anomalies WILL CHANGE as the project evolves: depending on how well the teams are doing, I will generate easier or harder anomalies**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **first round** will take place after recess (week 9): this means that I will **release the next test set on the tuesday of week 9**, and you must hand in your scores before the **WEDNESDAY at NOON (5th of October)**. We will then look at the results together on the thursday.  \n",
    "\n",
    "We will check everyone's performance in this way every week (once on  week 10, once on week 11 and once on week 12). \n",
    "\n",
    "Whilst performance (expressed in terms of AUC and your ranking compared to other teams) at **each of the check points** (weeks 9 to 12 inclusive) is an **important component** of your **final grade**, the **final report** and the detail of the various methods you will have tried will **also** be very **important**. Ideally, to get perfect marks (A+), you should try at least **two supervised methods** and **two unsupervised methods**, as well as be ranked the **best team** in terms of performance.\n",
    "\n",
    "The performance part of the grading will be based half on performance at weeks 9,10,11 and half on performance at week 12. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load all previous weeks' data as training sets\n",
    "batch1=np.load(\"first_batch.npz\")\n",
    "batch2=np.load(\"second_batch_with_labels.npz\")\n",
    "batch3=np.load(\"third_batch_with_labels.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1=pd.DataFrame(batch1[\"X\"], columns=[\"user\", \"item\", \"rating\"])\n",
    "y1=pd.DataFrame(batch1[\"y\"], columns=[\"user\", \"label\"])\n",
    "X2=pd.DataFrame(batch2[\"X\"], columns=[\"user\", \"item\", \"rating\"])\n",
    "y2=pd.DataFrame(batch2[\"y\"], columns=[\"user\", \"label\"])\n",
    "X3=pd.DataFrame(batch3[\"X\"], columns=[\"user\", \"item\", \"rating\"])\n",
    "y3=pd.DataFrame(batch3[\"y\"], columns=[\"user\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(X: pd.DataFrame):\n",
    "  features = pd.DataFrame(X.groupby('user')['user'].max())\n",
    "  # features['mode'] = X.groupby('user')['rating'].apply(lambda x: x.mode())\n",
    "  features['mean'] = X.groupby('user')['rating'].mean()\n",
    "  features['median'] = X.groupby('user')['rating'].median()\n",
    "  features['max'] = X.groupby('user')['rating'].max()\n",
    "  features['min'] = X.groupby('user')['rating'].min()\n",
    "  features['mode'] = X.groupby('user')['rating'].apply(lambda x: x.value_counts().index[0])\n",
    "  return features\n",
    "\n",
    "X1_features = get_features(X1)\n",
    "X2_features = get_features(X2)\n",
    "X3_features = get_features(X3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bab08701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Applying logreg and SVM models for binary classification\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn import svm\n",
    "# Evaluation metrics: AUC, prec, recall, F1\n",
    "from sklearn.metrics import auc, roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate logreg model and fit logreg\n",
    "logreg=LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "\n",
    "# Fit on first, second and third batch data\n",
    "logreg.fit(X1_features, y1[\"label\"])\n",
    "logreg.fit(X2_features, y2[\"label\"])\n",
    "logreg.fit(X3_features, y3[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user      1300\n",
       "item       990\n",
       "rating       6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and predict on fourth batch\n",
    "batch4 = np.load('fourth_batch.npz')\n",
    "X4 = pd.DataFrame(batch4['X'], columns=[\"user\", \"item\", \"rating\"])\n",
    "X4.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d9a0db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X4_features = get_features(X4)\n",
    "\n",
    "y_pred_logreg = logreg.predict(X4_features)\n",
    "y_pred_proba = logreg.predict_proba(X4_features)[::,1]\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "# print(\"------Logistic regression evaluation metrics------\")\n",
    "# print(f\"AUC: {roc_auc_score(y_test, y_pred_proba)}\")\n",
    "# print(f\"Precision: {precision_score(y_test, y_pred_logreg)}\")\n",
    "# print(f\"Recall: {recall_score(y_test, y_pred_logreg)}\")\n",
    "# print(f\"F1: {f1_score(y_test, y_pred_logreg)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4fc30bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.120336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.913030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.128706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.104525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.107561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>0.501180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>0.443208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297</th>\n",
       "      <td>0.489201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>0.814640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>0.447776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1300 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        y_pred\n",
       "0     0.120336\n",
       "1     0.913030\n",
       "2     0.128706\n",
       "3     0.104525\n",
       "4     0.107561\n",
       "...        ...\n",
       "1295  0.501180\n",
       "1296  0.443208\n",
       "1297  0.489201\n",
       "1298  0.814640\n",
       "1299  0.447776\n",
       "\n",
       "[1300 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pd.DataFrame(y_pred_proba, columns=['y_pred'])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f745d966",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('W11_predictions_emmanuel.npz', predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ec99a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('3.10.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "418913cb93534b0d0cf5f0fefdeb6f7e72048ef16d0da548eb476d106ddc4cf0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
